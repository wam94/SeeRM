name: External Intel (News)

on:
  workflow_dispatch:
    inputs:
      callsigns:
        description: "Optional: comma-separated callsigns to limit run"
        required: false
        type: string
      preview_only:
        description: "Preview (no email) — true/false"
        required: false
        default: "true"
        type: choice
        options: ["true","false"]
      lookback_days:
        description: "Intel lookback window (days)"
        required: false
        default: "10"
        type: string
      max_per_org:
        description: "Max items per org (after dedupe)"
        required: false
        default: "5"
        type: string
      send_to:
        description: "Override recipient (defaults to GMAIL_USER)"
        required: false
        type: string
      batch_size:
        description: "Orgs per batch (keeps jobs under timeout)"
        required: false
        default: "40"
        type: string
  schedule:
    - cron: "15 13 * * MON"  # Run 15 minutes after main workflow
  workflow_run:
    workflows: ["Weekly Digest"]
    types: [completed]

permissions:
  contents: read

concurrency:
  group: news-weekly-${{ github.ref }}
  cancel-in-progress: false

jobs:
  # 1) Compute how many batches we need
  prepare:
    runs-on: ubuntu-latest
    outputs:
      batch_list: ${{ steps.out.outputs.batch_list }}
      count: ${{ steps.out.outputs.count }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.11", cache: "pip" }
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Notion preflight (schema guard)
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY || '' }}
          NOTION_COMPANIES_DB_ID: ${{ secrets.NOTION_COMPANIES_DB_ID || '' }}
          NOTION_INTEL_DB_ID: ${{ secrets.NOTION_INTEL_DB_ID || '' }}
        run: |
          if [ -z "${NOTION_API_KEY}" ] || [ -z "${NOTION_COMPANIES_DB_ID}" ] || [ -z "${NOTION_INTEL_DB_ID}" ]; then
            echo "Skipping Notion preflight (missing NOTION_* secrets)."; exit 0
          fi
          python - <<'PY'
          import os, re, sys, requests

          def undash(x): return x.replace("-", "")
          def dash32(x):
              return f"{x[0:8]}-{x[8:12]}-{x[12:16]}-{x[16:20]}-{x[20:]}" if re.fullmatch(r"[0-9a-fA-F]{32}", x or "") else x

          token = os.environ["NOTION_API_KEY"]
          comp_id = os.environ["NOTION_COMPANIES_DB_ID"]
          intel_id = os.environ["NOTION_INTEL_DB_ID"]
          H = {"Authorization": f"Bearer {token}", "Notion-Version": "2022-06-28"}

          def get_db(dbid):
              for cand in (dbid, undash(dbid), dash32(undash(dbid))):
                  r = requests.get(f"https://api.notion.com/v1/databases/{cand}", headers=H, timeout=30)
                  if r.status_code == 200:
                      return r.json()
              raise SystemExit(f"❌ Could not fetch DB {dbid} (check sharing & ID)")

          comp = get_db(comp_id)
          intel = get_db(intel_id)
          print("✅ Companies DB:", comp.get("title",[{"plain_text":"(no title)"}])[0]["plain_text"])
          print("✅ Intel DB:", intel.get("title",[{"plain_text":"(no title)"}])[0]["plain_text"])

          props = intel.get("properties", {})
          def ptype(name): return props.get(name, {}).get("type")
          errors = []

          # Expected schema: Company=title, Callsign=relation->Companies DB
          if ptype("Company") != "title":
              errors.append(f"- Intel DB property 'Company' must be type 'title', found: {ptype('Company') or '(missing)'}")

          if ptype("Callsign") != "relation":
              errors.append(f"- Intel DB property 'Callsign' must be type 'relation', found: {ptype('Callsign') or '(missing)'}")
          else:
              rel_db = (props["Callsign"].get("relation") or {}).get("database_id")
              # Normalize IDs for comparison
              if rel_db and undash(rel_db) != undash(comp["id"]):
                  errors.append(f"- 'Callsign' relation points to a different DB ({rel_db}); expected Companies DB ({comp['id']}).")

          # Optional helpful checks
          for name, want in (("Summary","rich_text"), ("Date","date")):
              if ptype(name) not in (None, want):
                  print(f"⚠️  Hint: '{name}' is {ptype(name)}, usually '{want}' works best.")

          if errors:
              print("\n❌ Notion Intel DB schema mismatch:")
              for e in errors: print(e)
              print("\nFix one of these:")
              print("  1) Adjust your Intel DB properties to match: Company=title, Callsign=relation→Companies DB; or")
              print("  2) Update your code that writes Intel pages so it matches your actual schema.")
              sys.exit(1)
          else:
              print("✅ Intel DB schema looks good (Company=title, Callsign=relation to Companies).")
          PY
      - name: Compute batches
        id: out
        env:
          PYTHONPATH: ${{ github.workspace }}

          # Gmail (for roster CSV)
          GMAIL_CLIENT_ID: ${{ secrets.GMAIL_CLIENT_ID }}
          GMAIL_CLIENT_SECRET: ${{ secrets.GMAIL_CLIENT_SECRET }}
          GMAIL_REFRESH_TOKEN: ${{ secrets.GMAIL_REFRESH_TOKEN }}
          GMAIL_USER: ${{ secrets.GMAIL_USER }}

          PROFILE_SUBJECT: ${{ vars.NEWS_PROFILE_SUBJECT }}
          ATTACHMENT_REGEX: '.*\.csv$'

          INPUT_CALLSIGNS: ${{ inputs.callsigns }}
          BATCH_SIZE: ${{ inputs.batch_size }}
        run: |
          python - <<'PY'
          import os, io, json, pandas as pd
          from app.gmail_client import build_service, search_messages, get_message, extract_csv_attachments
          from app.news_job import fetch_csv_by_subject

          svc = build_service(
              client_id=os.environ["GMAIL_CLIENT_ID"],
              client_secret=os.environ["GMAIL_CLIENT_SECRET"],
              refresh_token=os.environ["GMAIL_REFRESH_TOKEN"],
          )
          user = os.environ.get("GMAIL_USER")
          # load roster csv by subject (same one news_job uses)
          df = fetch_csv_by_subject(svc, user, os.environ.get("PROFILE_SUBJECT") or "Org Profile — Will Mitchell")

          calls = []
          if df is not None and "callsign" in [c.lower() for c in df.columns]:
              cols = {c.lower(): c for c in df.columns}
              for _, r in df.iterrows():
                  cs = str(r[cols["callsign"]]).strip().lower()
                  if cs: calls.append(cs)
          calls = sorted(set(calls))

          # optional manual filter from workflow input
          filt = (os.environ.get("INPUT_CALLSIGNS") or "").strip()
          if filt:
              want = [c.strip().lower() for c in filt.split(",") if c.strip()]
              calls = [c for c in calls if c in want]

          n = len(calls)
          bs = int(os.environ.get("BATCH_SIZE","40") or "40")
          num_batches = (n + bs - 1) // bs if bs > 0 else 1
          # we'll recompute slicing in each run job; here we just emit range indices
          batch_list = list(range(num_batches))
          print(f"Roster total: {n} | batch_size={bs} -> {num_batches} batch(es)")

          # Handle case where no organizations found - create empty batch to prevent matrix failure
          if n == 0:
              print("⚠️  No organizations found in roster CSV - workflow will skip processing")
              batch_list = []  # Empty list will skip the run job entirely

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"batch_list={json.dumps(batch_list)}\n")
              f.write(f"count={n}\n")
          PY

  # 2) Process each batch in parallel (limited)
  run:
    runs-on: ubuntu-latest
    needs: prepare
    if: ${{ needs.prepare.outputs.count != '0' }}  # Skip if no organizations found
    timeout-minutes: 30              # per-batch timeout (keeps the long run from hitting 40)
    strategy:
      max-parallel: 2               # respect Notion rate limits; bump to 3–4 if safe
      fail-fast: false
      matrix:
        batch: ${{ fromJSON(needs.prepare.outputs.batch_list) }}

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.11", cache: "pip" }
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Build callsign slice for this batch
        id: slice
        env:
          PYTHONPATH: ${{ github.workspace }}

          # Gmail
          GMAIL_CLIENT_ID: ${{ secrets.GMAIL_CLIENT_ID }}
          GMAIL_CLIENT_SECRET: ${{ secrets.GMAIL_CLIENT_SECRET }}
          GMAIL_REFRESH_TOKEN: ${{ secrets.GMAIL_REFRESH_TOKEN }}
          GMAIL_USER: ${{ secrets.GMAIL_USER }}

          PROFILE_SUBJECT: ${{ vars.NEWS_PROFILE_SUBJECT }}
          ATTACHMENT_REGEX: '.*\.csv$'

          INPUT_CALLSIGNS: ${{ inputs.callsigns }}
          BATCH_SIZE: ${{ inputs.batch_size }}
          BATCH_INDEX: ${{ matrix.batch }}
        run: |
          python - <<'PY'
          import os, io, json, pandas as pd
          from app.gmail_client import build_service
          from app.news_job import fetch_csv_by_subject

          svc = build_service(
              client_id=os.environ["GMAIL_CLIENT_ID"],
              client_secret=os.environ["GMAIL_CLIENT_SECRET"],
              refresh_token=os.environ["GMAIL_REFRESH_TOKEN"],
          )
          user = os.environ.get("GMAIL_USER")
          df = fetch_csv_by_subject(svc, user, os.environ.get("PROFILE_SUBJECT") or "Org Profile — Will Mitchell")
          calls = []
          if df is not None and "callsign" in [c.lower() for c in df.columns]:
              cols = {c.lower(): c for c in df.columns}
              for _, r in df.iterrows():
                  cs = str(r[cols["callsign"]]).strip().lower()
                  if cs: calls.append(cs)
          calls = sorted(set(calls))

          # optional manual filter
          filt = (os.environ.get("INPUT_CALLSIGNS") or "").strip()
          if filt:
              want = [c.strip().lower() for c in filt.split(",") if c.strip()]
              calls = [c for c in calls if c in want]

          bs = int(os.environ.get("BATCH_SIZE","40") or "40")
          bi = int(os.environ.get("BATCH_INDEX","0") or "0")
          start, end = bi*bs, bi*bs + bs
          part = calls[start:end]
          head = ", ".join(part[:5]) + (f" … (+{max(0,len(part)-5)} more)" if len(part)>5 else "")
          print(f"Batch {bi}: {len(part)} orgs | head: {head}")

          # Output CSV string for env use
          with open("/tmp/filter_callsigns.txt","w") as f:
              f.write(",".join(part))
          PY

      - name: Run external intel for this batch
        env:
          PYTHONPATH: ${{ github.workspace }}

          # Gmail
          GMAIL_CLIENT_ID: ${{ secrets.GMAIL_CLIENT_ID }}
          GMAIL_CLIENT_SECRET: ${{ secrets.GMAIL_CLIENT_SECRET }}
          GMAIL_REFRESH_TOKEN: ${{ secrets.GMAIL_REFRESH_TOKEN }}
          GMAIL_USER: ${{ secrets.GMAIL_USER }}

          # Google CSE
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}

          # LLM
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_CHAT_MODEL: ${{ vars.OPENAI_CHAT_MODEL }}
          OPENAI_TEMPERATURE: ${{ vars.OPENAI_TEMPERATURE || '0.2' }}

          # Notion
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          NOTION_COMPANIES_DB_ID: ${{ secrets.NOTION_COMPANIES_DB_ID }}
          NOTION_INTEL_DB_ID: ${{ secrets.NOTION_INTEL_DB_ID }}

          # Email / intel knobs
          NEWS_GMAIL_QUERY: ${{ vars.NEWS_GMAIL_QUERY }}
          NEWS_PROFILE_SUBJECT: ${{ vars.NEWS_PROFILE_SUBJECT }}
          ATTACHMENT_REGEX: '.*\.csv$'
          PREVIEW_ONLY: ${{ inputs.preview_only }}
          INTEL_LOOKBACK_DAYS: ${{ inputs.lookback_days }}
          INTEL_MAX_PER_ORG: ${{ inputs.max_per_org }}
          DIGEST_TO: ${{ inputs.send_to || secrets.DIGEST_TO }}
          DIGEST_CC: ${{ secrets.DIGEST_CC }}
          DIGEST_BCC: ${{ secrets.DIGEST_BCC }}

          # Runtime/cost controls (tune to avoid timeouts)
          CSE_ONLY_IF_RSS_BELOW: ${{ vars.CSE_ONLY_IF_RSS_BELOW || '1' }}
          CSE_MAX_QUERIES_PER_ORG: ${{ vars.CSE_MAX_QUERIES_PER_ORG || '3' }}
          CSE_DISABLE_OWNER_QUERIES: ${{ vars.CSE_DISABLE_OWNER_QUERIES || 'false' }}
          CSE_DISABLE_TAG_QUERIES: ${{ vars.CSE_DISABLE_TAG_QUERIES || 'true' }}
          FETCH_ARTICLE_CONTENT: ${{ vars.FETCH_ARTICLE_CONTENT || 'false' }}
          FETCH_MAX_PER_ORG: ${{ vars.FETCH_MAX_PER_ORG || '2' }}
          ARTICLE_READ_TIMEOUT: ${{ vars.ARTICLE_READ_TIMEOUT || '8' }}
          ARTICLE_MAX_BYTES: ${{ vars.ARTICLE_MAX_BYTES || '250000' }}
        run: |
          export FILTER_CALLSIGNS="$(cat /tmp/filter_callsigns.txt)"
          echo "FILTER_CALLSIGNS for this batch: ${FILTER_CALLSIGNS}"
          python -m app.news_job

      # Optional: generate baselines only once (batch 0) for *new* callsigns
      - name: Read new callsigns (if any)
        if: ${{ matrix.batch == 0 }}
        id: newcs
        run: |
          if [ -f /tmp/new_callsigns.txt ]; then
            CS=$(cat /tmp/new_callsigns.txt)
            echo "callsigns=$CS" >> $GITHUB_OUTPUT
            echo "New callsigns: $CS"
          else
            echo "No new callsigns file found."
          fi

      - name: Generate baselines for new accounts
        if: ${{ matrix.batch == 0 && steps.newcs.outputs.callsigns != '' }}
        env:
          PYTHONPATH: ${{ github.workspace }}

          GMAIL_CLIENT_ID: ${{ secrets.GMAIL_CLIENT_ID }}
          GMAIL_CLIENT_SECRET: ${{ secrets.GMAIL_CLIENT_SECRET }}
          GMAIL_REFRESH_TOKEN: ${{ secrets.GMAIL_REFRESH_TOKEN }}
          GMAIL_USER: ${{ secrets.GMAIL_USER }}

          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_CHAT_MODEL: ${{ vars.OPENAI_CHAT_MODEL || 'gpt-5-mini' }}
          OPENAI_TEMPERATURE: ${{ vars.OPENAI_TEMPERATURE || '0.2' }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}

          NEWS_GMAIL_QUERY: ${{ vars.BASELINE_GMAIL_QUERY || vars.NEWS_GMAIL_QUERY }}
          NEWS_PROFILE_SUBJECT: ${{ vars.BASELINE_PROFILE_SUBJECT || vars.NEWS_PROFILE_SUBJECT }}
          ATTACHMENT_REGEX: '.*\.csv$'

          BASELINE_CALLSIGNS: ${{ steps.newcs.outputs.callsigns }}
          PREVIEW_ONLY: 'false'
          BASELINE_LOOKBACK_DAYS: ${{ vars.BASELINE_LOOKBACK_DAYS || '180' }}

          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          NOTION_COMPANIES_DB_ID: ${{ secrets.NOTION_COMPANIES_DB_ID }}
        run: |
          python -m app.dossier_baseline

  # 3) Summary job that always runs to ensure workflow success
  summary:
    runs-on: ubuntu-latest
    needs: [prepare]  # Only depend on prepare, run may be skipped
    if: always()  # Always run regardless of other job status
    steps:
      - name: Workflow Summary
        run: |
          echo "## 📊 News Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ "${{ needs.prepare.outputs.count }}" == "0" ]]; then
            echo "⚠️ **No organizations found in roster CSV**" >> $GITHUB_STEP_SUMMARY
            echo "- Roster fetch completed successfully but returned 0 organizations" >> $GITHUB_STEP_SUMMARY
            echo "- Check Gmail CSV with subject: '${{ vars.NEWS_PROFILE_SUBJECT || 'Org Profile — Will Mitchell' }}'" >> $GITHUB_STEP_SUMMARY
            echo "- Verify CSV contains 'callsign' column with valid data" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "✅ **Workflow completed successfully** (no processing needed)" >> $GITHUB_STEP_SUMMARY
          else
            echo "✅ **Organizations found: ${{ needs.prepare.outputs.count }}**" >> $GITHUB_STEP_SUMMARY
            echo "- News intelligence processing should have been triggered" >> $GITHUB_STEP_SUMMARY
            echo "- Check the run job status for processing results" >> $GITHUB_STEP_SUMMARY
          fi
